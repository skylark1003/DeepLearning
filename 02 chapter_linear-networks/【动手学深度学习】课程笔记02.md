---
title: 【动手学深度学习】课程笔记02-线性神经网络
date: 2023-03-01 20:35:13
tags:
- 深度学习
- 笔记
categories:
- 深度学习
toc: true
mathjax: true
---

# 线性回归

线性回归具体细节可以参考[【机器学习】课程笔记02_单变量线性回归(Linear Regression with One Variable)](https://skylark1003.github.io/2022/11/02/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%91%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B002/)

## 线性回归

- **线性模型：**

  - n维输入：$\mathbf x = [x_1, x_2, \cdots,x_n]^T$

  - 一个n维权重和一个标量偏差：$\mathbf w = [w_1, w_2, \cdots, w_n]^T, \quad b$

  - 输出是输入的加权和：$y = w_1x_1+w_2x_2+\cdots+w_nx_n+b$

    向量版本：$y = \langle \mathbf w, \mathbf x \rangle + b$

- **线性模型可以看作单层神经网络**

- **神经网络源于神经科学**

- **衡量预估值：**

  - 平方损失：$\ell(y,\hat{y})=\frac{1}{2}(y-\hat{y})^2$（$y$ 是真实值，$\hat{y}$ 是预估值）

- **训练数据：**

  - 收集数据点（训练数据）决定参数值（权重和偏差），通常越多越好。
  - n个样本：$\mathbf X=[\mathbf x_1,\mathbf x_2,\cdots,\mathbf x_n]^T \quad \mathbf  y=[y_1, y_2,\cdots,y_n]^T$

- **参数学习：**

  - 训练损失：$\ell (\mathbf X,\mathbf y,\mathbf w,b)=\cfrac{1}{2n}\displaystyle\sum^n_{i=1}(y_i-\langle\mathbf x_i,\mathbf w\rangle-b)^2=\cfrac{1}{2}\lVert\mathbf y-\mathbf W \mathbf w-b \rVert^2$
  - 最小化损失来学习参数：$\mathbf{w}^{*}, \mathbf{b}^{*}=\arg \displaystyle \min _{\mathbf{w}, b} \ell(\mathbf{X}, \mathbf{y}, \mathbf{w}, b)$

- **显示解：**

  - 将偏差加入权重：$\mathbf{X} \leftarrow[\mathbf{X}, \mathbf{1}] \quad \mathbf{w} \leftarrow\left[\begin{array}{l}
    \mathbf{w} \\ b \end{array}\right]$

    $\ell(\mathbf X,\mathbf y,\mathbf w)=\cfrac{1}{2n}\lVert\mathbf y-\mathbf X\mathbf w\rVert^2 \quad \cfrac{\partial}{\partial\mathbf w}\ell(\mathbf X,\mathbf y,\mathbf w)=\cfrac{1}{n}(\mathbf y-\mathbf X\mathbf w)^T\mathbf X$

  - 损失是凸函数，所以最优解满足：

    $\cfrac{\partial}{\partial\mathbf w}\ell(\mathbf X,\mathbf y,\mathbf w)=0 \ \Leftrightarrow \ \cfrac{1}{n}(\mathbf y-\mathbf X\mathbf w)^T\mathbf x = 0 \ \Leftrightarrow \ \mathbf w^*=(\mathbf X^T\mathbf X)^{-1}\mathbf X^T\mathbf y$

- **总结：**

  - 线性回归是对n维输入的加权，外加偏差
  - 使用平方损失来衡量预测值和真实值的差异
  - 线性回归有显示解
  - 线性回归可以看作单层神经网络

## 基础优化算法

- **梯度下降：**
  - 挑选一个初始值 $\mathbf w_0$
  - 重复迭代参数 t=1,2,3  $\mathbf w_t = \mathbf w_{t-1}-\eta\cfrac{\partial\ell}{\partial\mathbf w_{t-1}}$
  - 沿梯度方向将增加损失函数值
  - 学习率：步长的超参数
- **选择学习率：**
  - 不能太大也不能太小
- **小批量随机梯度下降：**
  - 在整个训练集上算梯度太贵（一个深度神经网络模型可能需要数分钟至数小时）
  - 随机采样b个样本$i_1,i_2,\cdots,i_b$来近似损失：$\cfrac{1}{b}\displaystyle\sum_{i\in I_b}\ell(\mathbf x_i,y_i,\mathbf w)$
    - b是批量大小，另一个重要的超参数
  - 选择批量：
    - 不能太小：不适合并行来最大利用资源
    - 不能太大：内存消耗增加，浪费计算
- **总结：**
  - 梯度下降通过不断沿着反梯度方向更新参数求解
  - 小批量随机梯度下降是深度学习默认的求解算法
  - 两个重要的超参数是**批量大小**和**学习率**

## 线性回归从零开始实现

- **生成数据集：**

  - 我们生成一个包含1000个样本的数据集， 每个样本包含从标准正态分布中采样的2个特征。 我们的合成数据集是一个矩阵 $\mathbf X \in \Bbb R^{1000 \times 2}$。

  - 使用线性模型参数 $\mathbf w = [2, -3.4]^T 、 b = 4.2$ 和噪声项 $\epsilon$ 生成数据集及其标签：
    $$
    \mathbf y = \mathbf X \mathbf w + b + \epsilon
    $$
    $\epsilon$ 可以视为模型预测和标签是的潜在观测误差，我们认为标准假设成立，即 $\epsilon$ 服从均值维0的正态分布。

    ```python
    import random
    import torch
    from d2l import torch as d2l
    
    def synthetic_data(w, b, num_examples):
        # 生成 y = Xw + b + 噪声
        # 生成 X：均值为0，方差为1的随机数，大小为 样本数 × w长度
        X = torch.normal(0, 1, (num_examples, len(w)))
        # 生成 y
        y = torch.matmul(X, w) + b
        # 为了让问题难一点，加一个均值为0，方差为0.01的随机噪音
        y += torch.normal(0, 0.01, y.shape)
        # 将 X 和 y 转为列向量返回
        return X, y.reshape((-1, 1))     # -1 表示Numpy根据维度自动计算数组的shape属性值
    
    true_w = torch.tensor([2, -3.4])     # 真实的 w
    true_b = 4.2                         # 真实的 b
    # 生成特征和标注，获得训练样本
    features, labels = synthetic_data(true_w, true_b, 1000)
    # 第0个样本：特征为长为2的向量，标注为标量
    print('features:', features[0], '\nlabel:', labels[0])
    # 显示
    d2l.set_figsize()                    # 设置图表大小
    # 绘制散点图
    # 在pytorch部分版本中需要从计算图中detach才能显示
    d2l.plt.scatter(features[:, 1].detach().numpy(), labels.detach().numpy())
    # 显示
    d2l.plt.show()
    ```

- **读取数据集：**

  - 训练模型时要对数据集进行遍历，每次抽取一小批量样本，并使用它们来更新我们的模型。所以有必要定义一个函数， 用于**打乱数据集中的样本并以小批量方式获取数据**。

    ```python
    # 输入：批量大小、矩阵特征、标签向量
    # 输出：大小为batch_size的小批量
    def data_iter(batch_size, features, labels):
        num_examples = len(features)
        # 生成每个样本的indices（0-n-1），转成python的list
        indices = list(range(num_examples))
        # 随机打乱
        random.shuffle(indices)
        # 从 0 到 num_examples，每次跳 batch_size 的大小
        for i in range(0, num_examples, batch_size):
            # 每次从 i 开始，到 i+batch_size。最后一批可能出现未满情况，取 i+batch_size 和 num_examples 最小值
            # 该 batch_indices 为随机的
            batch_indices = torch.tensor(indices[i:min(i + batch_size, num_examples)])
            # 每次随机产生
            # python的一个生成器（generator)，不断调用不断返回，直到全部完成
            yield features[batch_indices], labels[batch_indices]
    
    batch_size = 10
    for X, y in data_iter(batch_size, features, labels):
        print(X, '\n', y)
        break
    ```

- **初始化模型参数：**

  - 在我们开始用小批量随机梯度下降优化我们的模型参数之前， 我们需要先有一些参数。

    ```python
    # 通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重
    w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)
    # 偏置初始化为0
    b = torch.zeros(1, requires_grad=True)
    ```

  - 初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据。

- **定义模型：**

  ```python
  def linreg(X, w, b):
      # 线性回归模型
      return torch.matmul(X, w) + b     # 广播
  ```

- **定义损失函数：**

  ```python
  def squared_loss(y_hat, y):
      # 均方损失
      return (y_hat - y.reshape(y_hat.shape))**2 / 2
  ```

- **定义优化算法：**

  ```python
  # 输入：参数（包括w和b）、学习率、批量大小
  def sgd(params, lr, bath_size):
      # 小批量随机梯度下降
      # 在该模块下，所有计算得出的tensor的requires_grad都自动设置为False。
      with torch.no_grad():
          for param in params:
              param -= lr * param.grad / bath_size    # 之前损失函数未求均值，这里求均值
              param.grad.zero_()                      # 梯度设为0
  ```

- **训练：**

  - 步骤：

    - 初始化参数：学习率、迭代周期（超参数需要通过反复试验进行调整）。
    - 重复以下训练，直到完成：
      - 读取一小批量训练样本，并通过模型获得一组预测。
      - 计算完损失后，我们开始反向传播，存储每个参数的**梯度**。 
      - 最后，调用优化算法`sgd`来**更新模型参数**。

    ```python
    lr = 0.05              # 学习率
    num_epochs = 5         # 迭代周期（整个数据扫描次数）
    net = linreg           # 目的：方便换成别的模型
    loss = squared_loss    # 在该模型中为均方损失
    
    # 两层 for loop （迭代周期 批量）
    for epoch in range(num_epochs):
        # 每次拿出批量大小的 X 和 y
        for X, y in data_iter(batch_size, features, labels):
            # net() 做预测  loss() 做损失
            l = loss(net(X, w, b), y)
            # 求和之后算梯度
            l.sum().backward()
            # 用 sgd 对 w 和 b 进行更新
            sgd([w, b], lr, batch_size)
        # 评价精度
        with torch.no_grad():
            train_1 = loss(net(features, w, b), labels)
            print(f' epoch {epoch + 1}, loss {float(train_1.mean()):f}')
    
    # 我们拥有真实参数，可以进行比较
    print(f'w的估计误差：{true_w - w.reshape(true_w.shape)}')
    print(f'b的估计误差：{true_b - b}')
    ```

## 线性回归的简洁实现

- **生成数据集：**

  ```python
  import torch
  # 引入一些处理数据的模块
  from torch.utils import data
  from d2l import torch as d2l
  
  true_w = torch.tensor([2, -3.4])
  true_b = 4.2
  features, labels = d2l.synthetic_data(true_w, true_b, 1000)
  ```

- **读取数据集：**

  ```python
  def load_array(data_arrays, batch_size, is_train=True):
      # 构造一个PyTorch数据迭代器
      # * 是元组拆包符号
      dataset = data.TensorDataset(*data_arrays)
      return data.DataLoader(dataset, batch_size, shuffle=is_train)
  
  batch_size = 10
  data_iter = load_array((features, labels), batch_size)
  
  print(next(iter(data_iter)))
  ```

- **定义模型：**

  ```python
  # 定义模型变量net，它是一个Sequential类的实例
  # Sequential：将层放到容器里，将多个层串联在一起
  # Linear：线性层（全连接层），指定输入输出维度
  net = nn.Sequential(nn.Linear(2, 1))
  ```

- **初始化模型参数：**

  ```python
  # net第0层，使用均值为0，方差为0.01的值替换 w（权重） 的真实值
  net[0].weight.data.normal_(0, 0.01)
  # net第0层，使用0替换 b（偏差） 的真实值
  net[0].bias.data.fill_(0)
  ```

- **定义损失函数：**

  ```python
  # 计算均方误差（平方L2范数）
  loss = nn.MSELoss()
  ```

- **定义优化算法：**

  ```python
  # 小批量随机梯度下降，传入参数和超参数字典（学习率）
  trainer = torch.optim.SGD(net.parameters(), lr=0.03)
  ```

- **训练：**

  - 在每个迭代周期里，我们将完整遍历一次数据集（`train_data`）， 不停地从中获取一个小批量的输入和相应的标签。 对于每一个小批量，会进行以下步骤:

    - 通过调用`net(X)`生成预测并计算损失`l`（前向传播）。
    - 通过进行反向传播来计算梯度。
    - 通过调用优化器来更新模型参数。

    为了更好的衡量训练效果，我们计算每个迭代周期后的损失，并打印它来监控训练过程。

    ```python
    num_epochs = 3
    for epoch in range(num_epochs):
        for X, y in data_iter:
            # net中包含w和b，不需要传
            l = loss(net(X), y)
            trainer.zero_grad()
            l.backward()
            # 进行模型更新
            trainer.step()
        l = loss(net(features), labels)
        print(f'epoch {epoch + 1}, loss {l:f}')
    
    w = net[0].weight.data
    print('w的估计误差：', true_w - w.reshape(true_w.shape))
    b = net[0].bias.data
    print('b的估计误差：', true_b - b)
    ```

# Softmax 回归

分类问题具体细节可以参考[【机器学习】课程笔记05_逻辑回归(Logistic Regression)](https://skylark1003.github.io/2022/11/10/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%91%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B005/)

## Softmax 回归

- **分类 vs 回归：**

  - 回归估计一个连续值
  - 分类预测一个离散类别，例如MNIST（手写数字识别）、ImageNet（自然物体识别）

- **Kaggle上的分类问题：**

  - 蛋白质分类
  - 恶意软件分类
  - 恶意Wikipedia评论

- **从回归到多类分类：**

  - 回归：
    - 但连续数值输出
    - 自然区间 $\Bbb R$
    - 跟真实值的区别作为损失
  - 分类：
    - 通常多个输出
    - 输出 i 是预测为第 i 类的置信度

- **从回归到多类分类——均方损失：**

  - 对类别进行一位有效编码（one-hot）

    - $$
      \mathbf y = [y_1, y_2,\cdots,y_n]^T \\
      y_1=
      \begin{cases}
      1 & if \ i=y \\
      0 & otherwise
      \end{cases}
      $$

  - 使用均方损失训练

  - 最大值最为预测：$\hat y = arg\displaystyle \max_i \ o_i$ （选择具有最大输出值的类别作为预测）

- **从回归到多类分类——无校验比例**

  - 对类别进行一位有效编码

  - 最大值为预测 $\hat y = arg\displaystyle \max_i \ o_i$

  - 需要更置信地识别正确类（大余量） $o_y-o_i \geq \triangle(y,i)$

  - 输出匹配概率（非负，和为1）

    - $$
      \hat {\mathbf y}=softmax(\mathbf o) \\
      \hat {y_i} = \cfrac{exp(o_i)}{\sum_kexp(o_k)}
      $$

    - 做指数的好处是使值非负

- **Softmax和交叉熵损失：**

  - 交叉熵通常用来衡量两个概率的区别 $H(\mathbf p, \mathbf q) = \displaystyle \sum_i -p_ilog(q_i)$

  - 将它作为损失 $l(\mathbf y,\hat {\mathbf y})=-\displaystyle \sum_i y_ilog\hat{y_i}=-log \hat {y_y}$

    >  最终损失只和正确的那一类对应的输出有关

  - 其梯度使真实概率和预测概率的差异 $\partial_{o_i}l(\mathbf y,\hat {\mathbf y})=softmax(\mathbf o)_i-y_i$

- **总结：**

  - Softmax回归是一个多类分类模型
  - 使用Softmax操作子得到每个类的预测置信度
  - 使用交叉熵来衡量预测和标号的区别

## 损失函数

- **L2 Loss**
  - 均方损失：$l(y,y')=\cfrac{1}{2}(y-y')^2$
- **L1 Loss**
  - 绝对值损失：$l(y,y')=|y-y'|$
- **Huber's Robust Loss**
  - Huber鲁棒损失：$l(y,y')=\begin{cases} |y-y'|-\frac{1}{2} & if \ |y-y'|>1 \\ \frac{1}{2}(y-y')^2 & otherwise \end{cases}$

## 图像分类数据集

- **读取数据集**

  ```python
  import torch
  import torchvision                   # 计算机视觉相关的库
  from torch.utils import data         # 读取数据小批量的函数
  from torchvision import transforms   # 对数据进行操作
  from d2l import torch as d2l
  
  d2l.use_svg_display()
  
  # 通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式，
  # 并除以255使得所有像素的数值均在0～1之间
  trans = transforms.ToTensor()
  # 训练数据集
  mnist_train = torchvision.datasets.FashionMNIST(
      root="../data", train=True, transform=trans, download=True)
  # 测试数据集
  mnist_test = torchvision.datasets.FashionMNIST(
      root="../data", train=False, transform=trans, download=True)
  
  # 训练集和测试集的大小
  print(len(mnist_train), len(mnist_test))
  # 第一张图片的形状
  print(mnist_train[0][0].shape)
  # 灰度图像通道数为1，像素为28×28
  
  # 用于在数字标签索引及其文本名称之间进行转换
  def get_fashion_mnist_labels(labels):  #@save
      """返回Fashion-MNIST数据集的文本标签"""
      text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
                     'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
      return [text_labels[int(i)] for i in labels]
  
  # 可视化这些样本
  def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save
      """绘制图像列表"""
      figsize = (num_cols * scale, num_rows * scale)
      _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)
      axes = axes.flatten()
      for i, (ax, img) in enumerate(zip(axes, imgs)):
          if torch.is_tensor(img):
              # 图片张量
              ax.imshow(img.numpy())
          else:
              # PIL图片
              ax.imshow(img)
          ax.axes.get_xaxis().set_visible(False)
          ax.axes.get_yaxis().set_visible(False)
          if titles:
              ax.set_title(titles[i])
      return axes
  
  # 训练数据集中前几个样本的图像及其相应的标签
  X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))
  show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y));
  # 显示
  d2l.plt.show()
  ```

- **读取小批量**

  ```python
  batch_size = 256
  
  def get_dataloader_workers():  #@save
      """使用4个进程来读取数据"""
      return 4
  
  train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True,
                               num_workers=get_dataloader_workers())
  
  timer = d2l.Timer()
  for X, y in train_iter:
      continue
  # 读一遍数据所用时间
  print(f'{timer.stop():.2f} sec')
  ```

- **整合所有组件**

  ```python
  # 用于获取和读取Fashion-MNIST数据集
  # 返回训练集和验证集的数据迭代器
  def load_data_fashion_mnist(batch_size, resize=None):  #@save
      """下载Fashion-MNIST数据集，然后将其加载到内存中"""
      trans = [transforms.ToTensor()]
      if resize:
          trans.insert(0, transforms.Resize(resize))
      trans = transforms.Compose(trans)
      mnist_train = torchvision.datasets.FashionMNIST(
          root="../data", train=True, transform=trans, download=True)
      mnist_test = torchvision.datasets.FashionMNIST(
          root="../data", train=False, transform=trans, download=True)
      return (data.DataLoader(mnist_train, batch_size, shuffle=True,
                              num_workers=get_dataloader_workers()),
              data.DataLoader(mnist_test, batch_size, shuffle=False,
                              num_workers=get_dataloader_workers()))
  
  # 通过指定resize参数来测试load_data_fashion_mnist函数的图像大小调整功能
  train_iter, test_iter = load_data_fashion_mnist(32, resize=64)
  for X, y in train_iter:
      print(X.shape, X.dtype, y.shape, y.dtype)
      break
  ```

## Softmax 回归的从零开始实现

- **初始化模型参数**

  ```python
  import torch
  from IPython import display
  from d2l import torch as d2l
  
  batch_size = 256
  # 返回训练集和测试集的迭代器
  train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
  
  # 将图片信息拉成向量 28×28=784
  num_inputs = 784
  # 10个分类类别
  num_outputs = 10
  
  # 初始化W和b
  W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)
  b = torch.zeros(num_outputs, requires_grad=True)
  ```

- **定义softmax操作**

  ```python
  X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
  # 按轴0求和
  print(X.sum(0, keepdim=True))
  # 按轴1求和
  print(X.sum(1, keepdim=True))
  
  # 实现softmax，对每一行做softmax
  def softmax(X):                               # 在该网络中：X行数为批量大小，列数为输入维数
      X_exp = torch.exp(X)                      # 01 对X求幂
      partition = X_exp.sum(1, keepdim=True)    # 02 对每一行求和
      return X_exp / partition                  # 03 利用广播机制获得 每个元素的指数/该行所有元素的指数和 即softmax
  
  # 测试softmax
  X = torch.normal(0, 1, (2, 5))
  X_prob = softmax(X)
  print(X_prob)
  print(X_prob.sum(1))
  ```

- **定义模型**

  ```python
  # 实现softmax回归模型
  def net(X):
      return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)
  ```

- **定义损失函数**

  ```python
  # y_hat的简单例子
  y = torch.tensor([0, 2])
  y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])
  # 把两个样本属于真实标签（0，2）的预测概率拿出来，即拿出来的是y_hat[0, 0]和y_hat[1, 2]
  print(y_hat[[0, 1], y])
  
  # 实现交叉熵损失函数（取真实值对应的预测概率）
  def cross_entropy(y_hat, y):
      return -torch.log(y_hat[range(len(y_hat)), y])
  print(cross_entropy(y_hat, y))
  ```

- **分类精度**

  ```python
  # 将预测类和真实y元素进行比较
  def accuracy(y_hat, y):
      """计算预测正确的数量。"""
      if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
          y_hat = y_hat.argmax(axis=1)        # 最大值下标 即预测的类别
      cmp = y_hat.type(y.dtype) == y          # 比较与真实类别是否一致
      return float(cmp.type(y.dtype).sum())   # 预测正确的数量
  
  print(accuracy(y_hat, y) / len(y))          # 预测正确的概率
  
  def evaluate_accuracy(net, data_iter):
      """计算在指定数据集上模型的精度。"""
      if isinstance(net, torch.nn.Module):
          net.eval()
      metric = Accumulator(2)
      for X, y in data_iter:
          metric.add(accuracy(net(X), y), y.numel())
      return metric[0] / metric[1]
  
  class Accumulator:
      """在`n`个变量上累加。"""
      def __init__(self, n):
          self.data = [0.0] * n
  
      def add(self, *args):
          self.data = [a + float(b) for a, b in zip(self.data, args)]
  
      def reset(self):
          self.data = [0.0] * len(self.data)
  
      def __getitem__(self, idx):
          return self.data[idx]
  
  print(evaluate_accuracy(net, test_iter))
  ```

- **训练**

  ```python
  # 训练
  def train_epoch_ch3(net, train_iter, loss, updater):
      """训练模型一个迭代周期（定义见第3章）。"""
      # 将模型设置为训练模式
      if isinstance(net, torch.nn.Module):
          net.train()
      # 训练损失综合、训练准确度总和、样本数
      metric = Accumulator(3)
      for X, y in train_iter:
          # 计算梯度并更新参数
          y_hat = net(X)
          l = loss(y_hat, y)
          if isinstance(updater, torch.optim.Optimizer):
              # 使用PyTorch内置的优化器和损失函数
              updater.zero_grad()
              l.backward()
              updater.step()
              metric.add(
                  float(l) * len(y), accuracy(y_hat, y),
                  y.size().numel())
          else:
              # 使用定制的优化器和损失函数
              l.sum().backward()
              updater(X.shape[0])
              metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
      # 返回训练损失和训练精度
      return metric[0] / metric[2], metric[1] / metric[2]
  
  class Animator:  #@save
      """在动画中绘制数据"""
      def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
                   ylim=None, xscale='linear', yscale='linear',
                   fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,
                   figsize=(3.5, 2.5)):
          # 增量地绘制多条线
          if legend is None:
              legend = []
          d2l.use_svg_display()
          self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)
          if nrows * ncols == 1:
              self.axes = [self.axes, ]
          # 使用lambda函数捕获参数
          self.config_axes = lambda: d2l.set_axes(
              self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
          self.X, self.Y, self.fmts = None, None, fmts
  
      def add(self, x, y):
          # 向图表中添加多个数据点
          if not hasattr(y, "__len__"):
              y = [y]
          n = len(y)
          if not hasattr(x, "__len__"):
              x = [x] * n
          if not self.X:
              self.X = [[] for _ in range(n)]
          if not self.Y:
              self.Y = [[] for _ in range(n)]
          for i, (a, b) in enumerate(zip(x, y)):
              if a is not None and b is not None:
                  self.X[i].append(a)
                  self.Y[i].append(b)
          self.axes[0].cla()
          for x, y, fmt in zip(self.X, self.Y, self.fmts):
              self.axes[0].plot(x, y, fmt)
          self.config_axes()
          display.display(self.fig)
          d2l.plt.draw();
          d2l.plt.pause(0.001)
          display.clear_output(wait=True)
  
  # 训练函数
  def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save
      """训练模型（定义见第3章）"""
      animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
                          legend=['train loss', 'train acc', 'test acc'])
      for epoch in range(num_epochs):
          train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
          test_acc = evaluate_accuracy(net, test_iter)
          animator.add(epoch + 1, train_metrics + (test_acc,))
      train_loss, train_acc = train_metrics
      assert train_loss < 0.5, train_loss
      assert train_acc <= 1 and train_acc > 0.7, train_acc
      assert test_acc <= 1 and test_acc > 0.7, test_acc
  
  lr = 0.1
  
  def updater(batch_size):
      return d2l.sgd([W, b], lr, batch_size)
  
  num_epochs = 10
  train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)
  d2l.plt.show()
  ```

  - ![](https://skylark-blog.oss-cn-beijing.aliyuncs.com/img/dl-02-01.png)

- **预测**

  ```python
  def predict_ch3(net, test_iter, n=6):  #@save
      """预测标签（定义见第3章）"""
      for X, y in test_iter:
          break
      trues = d2l.get_fashion_mnist_labels(y)
      preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))
      titles = [true +'\n' + pred for true, pred in zip(trues, preds)]
      d2l.show_images(
          X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])
      d2l.plt.show()
  
  predict_ch3(net, test_iter)
  ```

  - ![](https://skylark-blog.oss-cn-beijing.aliyuncs.com/img/dl-02-02.png)

## Softmax 回归的简洁实现

- **初始化模型参数**

  ```python
  import torch
  from torch import nn
  from d2l import torch as d2l
  
  batch_size = 256
  train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
  
  # PyTorch不会隐式地调整输入的形状。
  # 因此，我们在线性层前定义了展平层（flatten），来调整网络输入的形状
  # Flatten：把任何维度的tensor展平成一维tensor（展平图片）
  net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))
  
  def init_weights(m):
      if type(m) == nn.Linear:
          nn.init.normal_(m.weight, std=0.01)
  
  net.apply(init_weights);
  ```

- **重新审视Softmax的实现**

  - 指数可能会造成数值稳定问题，即**上溢**，其中一个解决技巧是在计算前从所有 $o_k$ 中减去 $\max(o_k)$ ，但可能出现接近零的值，即**下溢**。

  - 尽管我们要计算指数函数，但我们最终在计算交叉熵损失时会取它们的对数。 通过将softmax和交叉熵结合在一起，可以避免反向传播过程中可能会困扰我们的数值稳定性问题。

  - ```python
    loss = nn.CrossEntropyLoss()
    ```

- **优化算法**

  - ```python
    trainer = torch.optim.SGD(net.parameters(), lr=0.1)
    ```

- **训练**

  ```python
  num_epochs = 10
  d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
  d2l.plt.show()
  ```

  - ![](https://skylark-blog.oss-cn-beijing.aliyuncs.com/img/dl-02-03.png)

<br/>

***

课程链接：[动手学深度学习在线课程](https://courses.d2l.ai/zh-v2/)
