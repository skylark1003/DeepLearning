---
title: 【动手学深度学习】课程笔记01-预备知识
date: 2023-02-21 23:40:55
tags:
- 深度学习
- 笔记
categories:
- 深度学习
toc: true
mathjax: true
---

# 课程目标

- 介绍深度学习经典和最新的模型：LeNet、ResNet、LSTM、BERT……
- 机器学习基础：损失函数、目标函数、过拟合、优化
- 实践：
  - 使用Pytorch实现介绍的知识点
  - 在真实数据上体验算法效果

# 课程内容

- 深度学习基础：线性神经网络、多层感知机
- 卷积神经网络：LeNet、AlexNet、VGG、Inception、ResNet
- 循环神经网络：RNN、GRU、LSTM、seq2seq
- 注意力机制：Attention、Transformer
- 优化算法：SGD、Momentum、Adam
- 高性能计算：并行、多GPU、分布式
- 计算机视觉：目标检测、语义分割
- 自然语言处理：词嵌入、BERT

# 深度学习介绍

- 深度学习例子：
  - 图片分类
  - 物体检测与分割
  - 样式迁移
  - 人脸合成
  - 文字生成图片
  - 文字生成
  - 无人驾驶
- 案例点击——广告点击
  - 触发 -> 点击率预估 -> 排序（点击率×竞价）
  - 预测与训练：
    - 预测：特征提取 -> 模型 -> 模型预测
    - 训练：训练数据（过去广告展现和用户点击） -> 特征和用户点击 -> 模型
  - 完整过程：
    - 领域专家：模型控制广告展现，数据又用来训练新的模型。机器学习模型对产品的影响。
    - 数据科学家：将数据训练成模型。
    - AI专家：提升模型精度和性能。

# 安装

- **CUDA**

  - 在[NVIDIA官网](https://developer.nvidia.com/cuda-11-7-0-download-archive)下载与Pytorch版本相匹配的CUDA版本。

    ![](https://skylark-blog.oss-cn-beijing.aliyuncs.com/img/dl-01-01.png)

  - 在cmd中使用`nvidia-smi`命令查看是否安装成功，并可查看其版本号。

    ![](https://skylark-blog.oss-cn-beijing.aliyuncs.com/img/dl-01-03.png)

- **Miniconda**

  - 在[conda官网](https://docs.conda.io/en/latest/miniconda.html)下载Miniconda。

    ![](https://skylark-blog.oss-cn-beijing.aliyuncs.com/img/dl-01-02.png)

- **Pytorch**

  - 在[Pytorch官网](https://pytorch.org/get-started/locally/)获得下载命令，在Anaconda Prompt（miniconda3）中下载。

    ![](https://skylark-blog.oss-cn-beijing.aliyuncs.com/img/dl-01-04.png)
    
  - 下载后可以测试是否安装成功。
  
    ![](https://skylark-blog.oss-cn-beijing.aliyuncs.com/img/dl-01-05.png)
  
- **测试案例（ResNet）**：

  - 在[动手学深度学习](https://zh-v2.d2l.ai/)中下载Jupyter记事本并解压用于运行测试

  - 在conda中安装jupyter和d2l：`pip install jupyter d2l`
    - 在安装d2l时报错，**解决**：在[Python d2l项目包的下载文件](Python d2l项目包的下载文件)中下载**d2l-0.15.1-py3-none-any.whl**，进入该下载目录，运行命令`pip install d2l-0.15.1-py3-none-any.whl`。使用命令`pip list`，可以看到d2l下载成功。

  - 运行`jupyter notebook`进入jupyter
  
  - 打开文件：**/pytorch/chapter_convolutional-modern/resnet.ipynb**
  
  - **Kernel -> Restart & Run All** 在文件末尾可以看到正在运行
  
  - 运行结果：
  
    ![](https://skylark-blog.oss-cn-beijing.aliyuncs.com/img/dl-01-06.png)

- **Pycharm中Pytorch环境配置**
  
  - 在cmd中执行Pytorch安装命令（见上）
    - 若提示未找到版本，说明Python版本过高，卸载重装低版本Python。（我卸载了python11.0重新安装了python3.8.7）

# 数据操作

- **N维数组样例：**
  
  - 0-d（标量）：一个类别
  - 1-d（向量）：一个特征向量
  - 2-d（矩阵）：一个样本——特征矩阵
  - 3-d：RGB图片（宽×高×通道）
  - 4-d：一个RGB图片批量（批量大小×宽×高×通道）
  - 5-d：一个视频批量（批量大小×时间×宽×高×通道）
  
- **创建数组：**
  
  - 形状：例如3×4矩阵
  - 每个元素的数据类型：例如32位浮点数
  - 每个元素的值：例如全为0或随机数
  
- **访问元素：**
  
  - 一个元素：[1, 2]
  - 一行元素：[1, :]
  - 一列元素：[:, 1]
  - 子区域：[1:3, 1:]       （开区间，1-2行，1到末尾列） 
  - 子区域：[::3, ::2]       （每三行一跳，每两列一跳）
  
- **代码实现：**

  ```python
  import torch
  # 1.入门
  x = torch.arange(12)   # 创建张量（由一个数值组成的数组）
  print(x)
  print(x.shape)         # 张量形状
  print(x.numel())       # 张量元素总数
  X = x.reshape(3, 4)    # 改变形状不改变元素数量和元素值
  print(X)
  
  a = torch.zeros((2, 3, 4))   # 全0
  b = torch.ones((2, 3, 4))    # 全1
  c = torch.randn(3, 4)        # 随机
  print(a)
  print(b)
  print(c)
  
  d = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])  # 指定内容
  print(d)
  
  # 2.运算符
  # 标准算术运算符，按元素运算
  x = torch.tensor([1.0, 2, 4, 8])  # 1.0表示为浮点数
  y = torch.tensor([2, 2, 2, 2])
  print(x + y)
  print(x - y)
  print(x * y)
  print(x / y)
  print(x ** y)
  print(torch.exp(x))  # 指数运算
  
  # 多个张量连结
  X = torch.arange(12, dtype=torch.float32).reshape((3, 4))
  Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
  print(torch.cat((X, Y), dim=0))   # 按0轴
  print(torch.cat((X, Y), dim=1))   # 按1轴
  
  print(X == Y)    # 通过逻辑运算符构建二元张量
  print(X.sum())   # 元素求和
  
  # 3.广播机制（broadcasting mechanism）
  a = torch.arange(3).reshape((3, 1))
  b = torch.arange(2).reshape((1, 2))
  print(a)
  print(b)
  print(a + b)     # 广播 分别按行按列复制
  
  # 4.索引和切片
  print(X)
  print(X[-1])         # 读
  print(X[1:3])        # 读
  print(X[1:3, 0:2])   # 读
  X[1, 2] = 9          # 写
  print(X)
  X[0:2, :] = 12       # 写
  print(X)
  
  # 5.节省内存
  before = id(Y)          # id()表示该object在python中的唯一标识号
  Y = Y + X
  after1 = id(Y)
  print(after1 == before)  # 为新结果分配内存，之前的Y被析构掉了
  Y += X
  after2 = id(Y)
  print(after2 == after1)  # 执行原地操作（可以减少内存开销）
  
  Z = torch.zeros_like(Y)  # zeros_like(): 数据类型和Y一样但值均为0
  print("id(Z):", id(Z))
  Z[:] = X + Y             # 执行原地操作
  print("id(Z):", id(Z))
  
  # 6.转换为其他Python对象
  A = X.numpy()            # 转换为NumPy
  B = torch.tensor(A)
  print(type(A))
  print(type(B))
  
  a = torch.tensor([3.5])
  print(a)
  print(a.item())
  print(float(a))
  print(int(a))
  ```

# 数据预处理

- **代码实现：**

  ```python
  import os    # 常用路径操作、进程管理、环境参数等几类
  # 1.读取数据集
  os.makedirs(os.path.join('..', 'data'), exist_ok=True)
  data_file = os.path.join('..', 'data', 'house_tiny.csv')    # 创建csv文件
  with open(data_file, 'w') as f:
      f.write('NumRooms,Alley,Price\n')   # 列名
      f.write('NA,Pave,127500\n')         # 每行表示一个数据样本
      f.write('2,NA,106000\n')
      f.write('4,NA,178100\n')
      f.write('NA,NA,140000\n')
  
  import pandas as pd
  data = pd.read_csv(data_file)           # 从创建的csv文件中加载原始数据集
  print(data)
  
  # 2.处理缺失值
  inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]
  inputs = inputs.fillna(inputs.mean(numeric_only=True))    # 插值：空置用均值填充，非数值不操作
  print(inputs)
  
  # 对于inputs中的类别值或离散值，将NaN视为一个类别
  # Alley若为Pava则Alley_Pave为1，若为NaN则Alley_nan为1
  inputs = pd.get_dummies(inputs, dummy_na=True)
  print(inputs)
  
  # 3.转换为张量格式
  # inputs和outputs中所有条目均为数值，则可以转化为张量格式。
  import torch
  X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)
  print(X)
  print(y)
  ```

- **view、copy和reshape：**

  - view只能作用在连续的张量上（张量中元素的内存地址是连续的）。
  - 而reshape连续或非连续都可以。调用x.reshape的时候，如果x在内存中是**连续**的，那么x.reshape会返回一个**view**（原地修改，此时内存地址不变），如果x在内存中是**不连续**的，会返回一个**clone**（新的张量，这时候内存地址变了）。
  - 推荐的做法是，想要原地修改就直接view，否则就先clone()再改。

# 线性代数

- **标量：**

  - 简单操作：$c = a + b \quad c = a \cdot b \quad c = \sin a$

  - 长度：
    $$
    \lvert a \rvert = 
    \begin{cases}
    a &if \ a > 0 \\
    -a &if \ a < 0
    \end{cases}
    \\
    \lvert a + b \rvert \leq \vert a \rvert + \lvert b \rvert
    \\
    \lvert a \cdot b \rvert = \vert a \rvert \cdot \lvert b \rvert
    $$

- **向量：**

  - 简单操作：
    $$
    \begin{aligned}
    c = a + b \quad & where \ c_i = a_i + b_i \\
    c = \alpha \cdot b \quad & where \ c_i = \alpha b_i \\
    c = \sin a \quad & where \ c_i = \sin a_i
    \end{aligned}
    $$

  - 长度：
    $$
    \lVert a \rVert_2 = \bigg[\displaystyle \sum ^m _{i=1} a^2_i\bigg]^{\frac{1}{2}} \\
    \lVert a \rVert \geq 0 \ for \ all \ a \\
    \lVert a+b \rVert \leq \lVert a \rVert +\lVert b \rVert \\
    \lVert a \cdot b \rVert = \lvert a \rvert \cdot \lVert b \rVert
    $$

  - 点乘：$a^Tb = \displaystyle \sum _i a_ib_i$

  - 正交：$a^Tb = \displaystyle \sum _i a_i b_i = 0$

- **矩阵：**

  - 简单操作：
    $$
    \begin{aligned}
    C = A + B \quad & where \ C_{ij} = A_{ij} + B_{ij} \\
    C = \alpha \cdot B \quad & where \ C_{ij} = \alpha B_{ij} \\
    C = \sin A \quad & where \ C_{ij} = \sin A_{ij}
    \end{aligned}
    $$

  - 乘法（矩阵乘以向量）：$c = Ab \ where \ c_i = \displaystyle \sum _j A_{ij}b_j$

    > 可以视为**扭曲空间**：一个向量通过一个矩阵运算变成另外一个向量，将空间进行扭曲。

  - 乘法（矩阵乘以矩阵）：$C = AB \ where \ C_{ik} = \displaystyle \sum _j A_{ij}B_{jk}$

  - 范数：$c = A \cdot b \ hence \lVert c \rVert \leq \lVert A \rVert \cdot \lVert b \rVert$

    - 取决于如何衡量b和c的长度
    - 常见范数：
      - 矩阵范数：最小的满足的上面公式的值
      - Frobenius范数：$\lVert A \rVert _{Frob} = \bigg [ \displaystyle \sum _{ij} A^2_{ij}\bigg] ^{\frac {1}{2}}$

- **特殊矩阵：**

  - 对称和反对称：$A_{ij} = A_{ji} \quad and \quad A_{ij} = -A_{ji}$

  - 正定：$\lVert x \rVert ^2 = x^Tx \geq 0 \ generalizes \ to \ x^TAx \geq 0$

    > 特征值大于0的矩阵。

  - 正交矩阵：

    - 所有行都相互正交
    - 所有行都有单位长度：$U \ with \displaystyle \sum _j \ U_{ij}U_{kj} = \delta_{ik}$
    - 可以写成：$UU^T = 1$

  - 置换矩阵：$P \ where \ P_{ij} = 1 \ if \ and \ only \ if \ j=\pi(i) $

    > 置换矩阵是正交矩阵

  - 特征向量和特征值

    - 不被矩阵改变方向的向量：$Ax = \lambda x$
    - 对称矩阵总是可以找到特征向量

- **代码实现：**

  ```python
  import torch
  # 1.标量
  x = torch.tensor([3.0])
  y = torch.tensor([2.0])
  print(x + y)               # 标量运算
  print(x * y)
  print(x / y)
  print(x ** y)
  
  # 2.向量
  x = torch.arange(4)        # 向量可以视为标量值组成的列表
  print(x)
  print(x[3])                # 通过张量的索引访问任一元素
  print(len(x))              # 访问张量的长度
  print(x.shape)             # 只有一个轴的张量，形状只有一个元素
  
  # 3.矩阵
  A = torch.arange(20).reshape(5, 4)                    # 通过两个分量m和n创建一个形状为m×n的矩阵
  print(A)
  print(A.T)                 # 矩阵的转置
  B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])   # 对称矩阵
  print(B)
  print(B.T)
  print(B == B.T)
  
  # 4.张量（多轴数据结构）
  X = torch.arange(24).reshape(2, 3, 4)                 # 三维张量
  print(X)
  
  
  # 1.基本运算
  A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
  B = A.clone()              # 通过分配新内存，将A的一个副本分配给B
  print(A)
  print(A + B)
  print(A * B)               # 按元素乘法：哈达玛积 ⊙
  
  a = 2
  X = torch.arange(24).reshape(2, 3, 4)
  print(a + X)               # 广播
  print((a * X).shape)       # 广播
  
  # 2.降维
  x = torch.arange(4, dtype=torch.float32)
  print(x)
  print(x.sum())             # 计算元素的和
  
  A = torch.arange(40, dtype=torch.float32).reshape(2, 5, 4)
  A.reshape(2, 5, 4)
  print(A)
  print(A.shape)
  print(A.sum())                  # 计算任意张量的元素和
  A_sum_axis0 = A.sum(axis=0)     # 指定轴，通过求和降低维度
  print(A_sum_axis0)
  print(A_sum_axis0.shape)
  A_sum_axis1 = A.sum(axis=1)
  print(A_sum_axis1)
  print(A_sum_axis1.shape)
  A_sum_axis2 = A.sum(axis=[0, 1])
  print(A_sum_axis2)
  print(A_sum_axis2.shape)
  
  print(A.mean())                 # 求平均值
  print(A.sum() / A.numel())
  print(A.mean(axis=0))           # 按维度求平均值
  print(A.sum(axis=0) / A.shape[0])
  
  # 3.非降维求和
  sum_A = A.sum(axis=1, keepdim=True)       # 计算总和时保持轴数不变（求和维度值变为1）
  print(sum_A)
  print(A / sum_A)                # 通过广播将A除以A，所以须保持维度一致
  
  print(A)                        # 某个轴计算A元素的累计总和
  print(A.cumsum(axis=0))
  
  # 4.点积
  y = torch.ones(4, dtype=torch.float32)
  print(x)
  print(y)
  print(torch.dot(x, y))          # 向量向量积：相同位置的按元素乘积的和
  print(torch.sum(x * y))         # 等价于执行按元素乘法，然后进行求和
  
  A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
  print(A.shape)
  print(x.shape)
  print(A)
  print(x)
  print(torch.mv(A, x))           # 矩阵向量积：A m×n x n×1
  
  B = torch.ones(4, 3)
  print(A)
  print(B)
  print(torch.mm(A, B))           # 矩阵矩阵积：A 5×4 B 4×3
  
  # 5.范数
  u = torch.tensor([3.0, -4.0])
  print(torch.norm(u))            # 向量的L2范数（长度）
  print(torch.abs(u).sum())       # 向量的L1范数（绝对值求和）
  
  print(torch.norm(torch.ones((4, 9))))    # 矩阵的佛罗贝尼乌斯范数（Frobenius norm）：矩阵元素的平方和的平方根（将矩阵拉成向量的L2范数）
  ```

# 矩阵计算

- **标量导数：**

|       $y$        | $a$  |   $x^n$    | $exp(x)$ |    $log(x)$    | $sin(x)$ |
| :--------------: | :--: | :--------: | :------: | :------------: | :------: |
| $\cfrac{dy}{dx}$ | $0$  | $nx^{n-1}$ | $exp(x)$ | $\cfrac{1}{x}$ | $cos(x)$ |

|       $y$        |              $u+v$              |               $uv$                |        $y=f(u),u=g(x)$         |
| :--------------: | :-----------------------------: | :-------------------------------: | :----------------------------: |
| $\cfrac{dy}{dx}$ | $\cfrac{du}{dx}+\cfrac{dv}{dx}$ | $\cfrac{du}{dx}v+\cfrac{dv}{dx}u$ | $\cfrac{dy}{du}\cfrac{du}{dx}$ |

- **亚导数：**

  - 将倒数拓展到不可微的函数
    $$
    \frac {\partial |x|}{\partial x} = 
    \begin{cases}
    1 & if \ x>0 \\
    -1 & if \ x<0 \\
    a & if \ x=0,a \in [-1,1]
    \end{cases}
    $$

    $$
    \frac {\partial}{\partial x} max(x,0)= 
    \begin{cases}
    1 & if \ x>0 \\
    0 & if \ x<0 \\
    a & if \ x=0,a \in [0,1]
    \end{cases}
    $$

- **梯度：**

  - 将导数拓展到向量：

    |          |                   |                      标量                       |                            向量                            |                             矩阵                             |
    | :------: | :---------------: | :---------------------------------------------: | :--------------------------------------------------------: | :----------------------------------------------------------: |
    |          |                   |                    $x$ (1,)                     |                     $\mathbf x$ (n,1)                      |                      $\mathbf X$ (n,k)                       |
    | **标量** |     $y$ (1,)      |      $\cfrac{\partial y}{\partial x}$ (1,)      |      $\cfrac{\partial y}{\partial  \mathbf x}$ (1,n)       |       $\cfrac{\partial y}{\partial  \mathbf X}$ (k,n)        |
    | **向量** | $\mathbf y$ (m,1) | $\cfrac{\partial  \mathbf y}{\partial x}$ (m,1) |  $\cfrac{\partial  \mathbf y}{\partial  \mathbf x}$ (m,n)  |  $\cfrac{\partial  \mathbf y}{\partial  \mathbf X}$ (m,k,n)  |
    | **矩阵** | $\mathbf Y$ (m,l) | $\cfrac{\partial  \mathbf Y}{\partial x}$ (m,l) | $\cfrac{\partial  \mathbf Y}{\partial  \mathbf x}$ (m,l,n) | $\cfrac{\partial  \mathbf Y}{\partial  \mathbf X}$ (m,l,k,n) |

  - $$
    \mathbf x = \begin{bmatrix}x_1 \\ x_2 \\ \vdots \\ x_n  \end{bmatrix}  \quad \cfrac{\partial y}{\partial  \mathbf x}=[\cfrac{\partial y}{\partial x_1},\cfrac{\partial y}{\partial x_2},\cdots,\cfrac{\partial y}{\partial x_n}]
    $$

    - 梯度和等高线是正交的，即指向值变化最大的方向。

    - 样例：
  
      |                    $y$                    |      $a$      |                    $au$                    | $sum(\mathbf x)$ | $\lVert \mathbf x \rVert ^2$ |
      | :---------------------------------------: | :-----------: | :----------------------------------------: | :--------------: | :--------------------------: |
      | $\cfrac{\partial y}{\partial  \mathbf x}$ | $\mathbf 0^T$ | $a\cfrac{\partial u}{\partial  \mathbf x}$ |  $\mathbf 1^T$   |       $2 \mathbf x^T$        |
  
      |                    $y$                    |                            $u+v$                             |                             $uv$                             |       $\langle \mathbf u, \mathbf v \rangle$ （内积）        |
      | :---------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
      | $\cfrac{\partial y}{\partial  \mathbf x}$ | $\cfrac{\partial u}{\partial  \mathbf x} + \cfrac{\partial v}{\partial \mathbf x}$ | $\cfrac{\partial u}{\partial  \mathbf x} v+ \cfrac{\partial v}{\partial  \mathbf x} u$ | $\mathbf u ^T\cfrac{\partial  \mathbf v}{\partial  \mathbf x} + \mathbf v^T\cfrac{\partial  \mathbf u}{\partial  \mathbf x}$ |

  - $$
    \mathbf y = \begin{bmatrix}y_1 \\ y_2 \\ \vdots \\ y_n  \end{bmatrix}  \quad \cfrac{\partial  \mathbf y}{\partial  x}=\begin{bmatrix}\cfrac{\partial y_1}{\partial x} \\ \cfrac{\partial y_2}{\partial x} \\ \vdots \\ \cfrac{\partial y_n}{\partial x}\end{bmatrix}
    $$

    - 分子布局符号

  - $$
    \mathbf x = \begin{bmatrix}x_1 \\ x_2 \\ \vdots \\ x_n  \end{bmatrix} \quad 
    \mathbf y = \begin{bmatrix}y_1 \\ y_2 \\ \vdots \\ y_n  \end{bmatrix} \quad 
    \cfrac{\partial  \mathbf y}{\partial  \mathbf x} =\begin{bmatrix}\cfrac{\partial y_1}{\partial \mathbf x} \\ \cfrac{\partial y_2}{\partial \mathbf x} \\ \vdots \\ \cfrac{\partial y_n}{\partial \mathbf x}\end{bmatrix} 
    = \begin{bmatrix}\cfrac{\partial y_1}{\partial x_1} ,\cfrac{\partial y_1}{\partial x_2}, \cdots, \cfrac{\partial y_1}{\partial x_n}\\ \cfrac{\partial y_2}{\partial x_1},\cfrac{\partial y_2}{\partial x_2}, \cdots, \cfrac{\partial y_2}{\partial x_n} \\ \vdots \\ \cfrac{\partial y_n}{\partial x_1} ,\cfrac{\partial y_n}{\partial x_2}, \cdots, \cfrac{\partial y_n}{\partial x_n}\end{bmatrix}
    $$
    
    - 样例：
    
      |                    $\mathbf y$                     | $\mathbf a$ | $\mathbf x$ | $\mathbf {Ax}$ | $\mathbf x^T \mathbf A$ |
      | :------------------------------------------------: | :---------: | :---------: | :------------: | :---------------------: |
      | $\cfrac{\partial  \mathbf y}{\partial  \mathbf x}$ | $\mathbf 0$ | $\mathbf 1$ |  $\mathbf A$   |      $\mathbf A^T$      |
    
      |                    $\mathbf y$                     |                    $a \mathbf u$                    |                        $\mathbf{Au}$                         |                   $\mathbf u + \mathbf v$                    |
      | :------------------------------------------------: | :-------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
      | $\cfrac{\partial  \mathbf y}{\partial  \mathbf x}$ | $a\cfrac{\partial  \mathbf u}{\partial  \mathbf x}$ | $\mathbf A \cfrac{\partial  \mathbf u}{\partial  \mathbf x}$ | $\cfrac{\partial  \mathbf u}{\partial  \mathbf x} + \cfrac{\partial  \mathbf v}{\partial  \mathbf x}$ |

- **链式法则和自动求导**

# 自动求导

- **向量链式法则：**

  - 标量链式法则：$y=f(u),u=g(x) \quad \cfrac{\partial y}{\partial x} = \cfrac{\partial y}{\partial u}\cfrac{\partial u}{\partial x}$

  - 拓展到向量

    | $\cfrac{\partial y}{\partial \mathbf x} = \cfrac{\partial y}{\partial u}\cfrac{\partial u}{\partial \mathbf x}$ | $\cfrac{\partial y}{\partial \mathbf x} = \cfrac{\partial y}{\partial \mathbf u}\cfrac{\partial \mathbf u}{\partial \mathbf x}$ | $\cfrac{\partial \mathbf y}{\partial \mathbf x} = \cfrac{\partial \mathbf  y}{\partial \mathbf u}\cfrac{\partial \mathbf u}{\partial \mathbf x}$ |
    | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
    |                      (1,n)  (1,) (1,n)                       |                      (1,n)  (1,k) (k,n)                      |                    (m,n)    (m,k)  (k,n)                     |

  - 例子：

    ![](https://skylark-blog.oss-cn-beijing.aliyuncs.com/img/dl-01-07.png)

    ![](https://skylark-blog.oss-cn-beijing.aliyuncs.com/img/dl-01-08.png)

- **自动求导：**

  - 自动求导计算一个函数在指定值上的导数
  - 有别于：
    - 符号求导：计算$f'(x)$后带入求值
    - 数值求导：$\displaystyle \lim _{h \to 0} \cfrac{f(x+h)-f(x)}{h}$

- **计算图：**
  - 将代码分解为操作子
  - 将计算表示成一个无环图
  - 显示构造
  - 隐式构造
- **自动求导的两种模式：**
  - 链式法则
  - 正向积累：求复合函数的值，计算时会保存所有的中间变量。
  - 反向积累（反向传递）：求偏导数和梯度，去除不必要的枝。
    - 即要求梯度需要正反均算一遍

- **代码：**

  - **01 简单例子**

    - 对函数$y=2\mathbf x^T\mathbf x$关于列向量$\mathbf x$求导：

    ```python
    import torch
    x = torch.arange(4.0)
    print(x)
    # 计算之前需要一个地方存储梯度
    x.requires_grad_(True)  # 等价于 x = torch.arange(4.0, requires_grad=True)
    print(x.grad)           # 通过 x.grad 访问x的梯度，默认值为None
    y = 2 * torch.dot(x, x)
    print(y)
    # 通过反向传播函数来自动计算y关于x每个分量的梯度
    y.backward()            # 求导
    print(x.grad)           # 访问导数
    print(x.grad == 4 * x)
    ```

    - 计算$y = \mathbf x.sum()$：

    ```python
    # 在默认情况下，PyTorch会积累梯度，需要清除之前的值
    x.grad.zero_()          # 下划线表示重写内容
    y = x.sum()
    y.backward()
    print(x.grad)
    ```

  - **02 非标量变量的反向传播**

    - 深度学习中，我们的目的不是计算微分矩阵，而是批量中每个样本单独计算的偏导数之和。

    ```python
    # 对非标量调用 backward() 需要传入一个 gradient 参数，指定微分函数关于 self 的梯度
    x.grad.zero_()
    y = x * x
    y.sum().backward()      # 等价于 y.backward(torch.ones(len(x)))
    print(x.grad)
    ```

  - **03 分离计算**

    - 将某些计算移动到记录的计算图之外：

    ```python
    x.grad.zero_()
    y = x * x
    u = y.detach()          # 将u移动到计算图之外，即u是一个常数而不是关于x的函数，值为x*x
    z = u * x
    
    z.sum().backward()
    print(x.grad == u)
    
    x.grad.zero_()
    y.sum().backward()      # y依旧是关于x的函数
    print(x.grad == 2 * x)
    ```

  - **04 Python控制流的梯度计算**

    - 即使构建函数的计算图需要通过Python控制流（例如条件、循环或任意函数调用），仍然可以计算得到变量的梯度。

    ```python
    def f(a):
        b = a * 2
        while b.norm() < 1000:
            b = b * 2
        if b.sum() > 0:
            c = b
        else:
            c = 100 * b
        return c
    
    a = torch.randn(size=(), requires_grad=True)
    d = f(a)
    d.backward()
    # 对于任何a，存在某个常量标量k，使得f(a)=k*a
    # 其中k的值取决于输入a，因此可以用d/a验证梯度是否正确
    print(a.grad == d / a)
    ```

<br/>

***

课程链接：[动手学深度学习在线课程](https://courses.d2l.ai/zh-v2/)

